{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shiningsunnyday/Documents/GitHub/CS229Final/good_news_venv/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: once\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, f1_score, balanced_accuracy_score\n",
    "from inspect import signature\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "import warnings\n",
    "warnings.warn('once')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from scipy import sparse as sp\n",
    "import time\n",
    "\n",
    "news_csv = pd.read_csv('OnlineNewsPopularity/OnlineNewsPopularity.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     3,
     37,
     57
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x132f29128>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "urls = news_csv.url\n",
    "total_batches = (len(urls) + 100) // 100\n",
    "\n",
    "def scrape(num_batches):\n",
    "    for batch in range(num_batches):\n",
    "        news_scrape = []\n",
    "        batch_size = 100 if batch < len(urls) // 100 else len(urls) % 100\n",
    "        prog = round(100 * (batch / ((len(urls) + 100) / 100)))\n",
    "        print(\"batch\", batch, '%s%% complete' % (prog))\n",
    "        author_err, tag_err, base_err = 0, 0, 0\n",
    "        for i in range(batch_size):\n",
    "            url = urls[100 * batch + i]\n",
    "            try:\n",
    "                pg = urllib.request.urlopen(url)\n",
    "                pg_bytes = pg.read()\n",
    "                pg_html = pg_bytes.decode('utf8')\n",
    "                soup = BeautifulSoup(pg_html, \"html.parser\")\n",
    "                try:\n",
    "                    author = soup.find(\"span\", \"author_name\").a.get_text()\n",
    "                except AttributeError:\n",
    "                    author = \"\"\n",
    "                    author_err += 1\n",
    "                result = \"\".join(list(map(lambda x: x.get_text(), soup.find(\"section\", \"article-content\").find_all(\"p\"))))\n",
    "                try:\n",
    "                    tags = soup.find(\"footer\", \"article-topics\")\n",
    "                    tags = list(map(lambda x: x.get_text().lower(), soup.find(\"footer\", \"article-topics\").find_all(\"a\")))\n",
    "                except AttributeError:\n",
    "                    tags = []\n",
    "                    tag_err += 1\n",
    "            except:\n",
    "                base_err += 1\n",
    "                pass\n",
    "            news_scrape.append([url, author, result.replace('\\n', ''), ','.join(tags)])\n",
    "        f_write = open(\"news_contents.txt\", 'a')\n",
    "        np.savetxt(f_write, np.array(news_scrape), delimiter = '|', fmt = '%s')\n",
    "        f_write.close()\n",
    "        \n",
    "def scrape_title(num_batches):\n",
    "    for batch in range(num_batches):\n",
    "        batch_size = 100 if batch < len(urls) // 100 else len(urls) % 100\n",
    "        prog = round(100 * (batch / ((len(urls) + 100) / 100)))\n",
    "        print(\"batch\", batch, '%s%% complete' % (prog))\n",
    "        titles = []\n",
    "        for i in range(batch_size):\n",
    "            url = urls[100 * batch + i]\n",
    "            try:\n",
    "                pg = urllib.request.urlopen(url)\n",
    "                pg_bytes = pg.read()\n",
    "                pg_html = pg_bytes.decode('utf8')\n",
    "                soup = BeautifulSoup(pg_html, \"html.parser\")\n",
    "                titles.append([url, soup.find('title').get_text()])\n",
    "            except:\n",
    "                pass\n",
    "        f_write = open(\"news_title.txt\", 'a')\n",
    "        np.savetxt(f_write, np.array(titles), delimiter = '|', fmt = '%s')\n",
    "        f_write.close()   \n",
    "\n",
    "def word_model(path=\"glove.twitter.27B/glove.twitter.27B.50d.txt\"):\n",
    "    f = open(path,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "        \n",
    "    f.close()\n",
    "    return model\n",
    "\n",
    "g_model = word_model()\n",
    "\n",
    "shares_lookup = dict(zip(news_csv.url, news_csv.iloc[:,-1]))\n",
    "shares_dist = np.array(news_csv.iloc[:,-1])\n",
    "\n",
    "plt.plot(shares_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "label = lambda x: int(x > 1400)\n",
    "arr_labels = lambda x: 1*(x > 1400)\n",
    "y = arr_labels(shares_dist)\n",
    "\n",
    "def eda_shares(shares_dist, news_csv):\n",
    "    plt.figure(0)\n",
    "    plt.hist(shares_dist[1000:2000], bins=100, range=(0, 8000), color=\"red\")\n",
    "    # plt.vlines([1250,2500,5000], [0,0,0], [3500,1000,400])\n",
    "    plt.title('#Shares Density')\n",
    "    # plt.savefig(fname=\"./shares_dist.png\", transparent=True, format='png', bbox_inches='tight')\n",
    "\n",
    "    plt.figure(1)\n",
    "    dates_dist = np.array(news_csv.iloc[:,1])\n",
    "    plt.scatter(-dates_dist, shares_dist, color=\"red\")\n",
    "    plt.title('Article Age vs #Shares')\n",
    "    plt.savefig(fname=\"./time_shares.png\", dpi=500)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     8
    ]
   },
   "outputs": [],
   "source": [
    "f_read = open(\"news_contents.txt\", 'r')\n",
    "news_lines = f_read.readlines()\n",
    "f_title_read = open(\"good_news/news_title.txt\", 'r')\n",
    "news_titles = f_title_read.readlines()\n",
    "news_titles_dic = {title.split('|')[0]: title.split('|')[1].strip('\\n') for title in news_titles}\n",
    "news_lines[0].split('|')\n",
    "n = {news.split('|')[0]: news for news in news_lines}\n",
    "\n",
    "def metadic(news_csv, news_titles_dic, n):\n",
    "    news_metadic={}\n",
    "    for i in range(len(news_csv)):\n",
    "        url = news_csv.url[i]\n",
    "        if url in news_titles_dic:\n",
    "            news_metadic[url] = {\"title\": news_titles_dic[url].strip('\\n'), \n",
    "                              \"content\": n[url].split('|')[2],\n",
    "                              \"author\": n[url].split('|')[1],\n",
    "                              \"keywords\": n[url].split('|')[3].strip('\\n').split(',') if len(n[url].split('|')) > 3 else None}\n",
    "        else:\n",
    "            news_metadic[url] = {\"title\": None,\n",
    "                             \"content\": \"\",\n",
    "                             \"author\": None,\n",
    "                             \"keywords\": None}\n",
    "    return news_metadic        \n",
    "\n",
    "news_metadic = metadic(news_csv, news_titles_dic,n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# gets keywords from news_metadic\n",
    "# get_k=lambda news_metadic: [' '.join(k) if k != None else '' \n",
    "#                             for k in [news_metadic.get(url,[])['keywords'] \n",
    "#                             for url in news_metadic.keys()]]\n",
    "    \n",
    "# k_freq_idf = TfidfVectorizer(lowercase=True,stop_words='english',\n",
    "#                              analyzer='word',strip_accents='unicode',min_df=1)\n",
    "# key_tf = k_freq_idf.fit_transform(get_k(news_metadic))\n",
    "# svd = TruncatedSVD(1000,'arpack')\n",
    "# key_tf_svd = svd.fit_transform(key_tf)\n",
    "\n",
    "# np.savetxt('./keyw_tf_svd.txt',key_tf_svd)\n",
    "\n",
    "key_tf_svd=np.loadtxt('./keyw_tf_svd.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     2,
     5,
     10,
     19,
     28,
     30,
     49,
     71,
     76,
     87,
     104
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "swords = set(stopwords.words('english'))\n",
    "\n",
    "def text_attr(article, vocab_ref):   \n",
    "    for word in article.split():\n",
    "        return np.array([])\n",
    "def batch_text_process(metadic):\n",
    "    batch = news_csv[:batch_size]\n",
    "    text_attr_col = pd.DataFrame({'words': [text_attr(metadic[news_csv.iloc[i, 0]][0])\n",
    "                                            for i in range(len(metadic))]})\n",
    "    return news_csv.join(text_attr_col)\n",
    "def article_content_dic(content, pos_set):\n",
    "    dic_count = {}\n",
    "    for article in content:\n",
    "        pos_tags = nltk.pos_tag(nltk.word_tokenize(article))\n",
    "        nouns = set(filter(lambda x: x[1] in pos_set and x[1] not in swords, pos_tags))\n",
    "#         nouns = set(filter(lambda x: x[1] not in swords, pos_tags))\n",
    "        for noun in nouns:\n",
    "            dic_count[noun] = dic_count[noun]+1 if noun in dic_count else 1\n",
    "    return {word[0]: dic_count[word] for word in dic_count.keys() if dic_count[word] > 5}\n",
    "def article_vocab_matrix(vocab_index, content):\n",
    "    matrix = np.array([[0 for _ in range(len(vocab_index))] for _ in range(len(content))])\n",
    "    for i in range(len(content)):\n",
    "        for word in content[i].lower().split():\n",
    "            if word in vocab_index:\n",
    "                matrix[i][vocab_index[word]] += 1\n",
    "    return matrix\n",
    "tags = 'NN, NNS, NNP, NNPS, JJ, JJR, JJS, VB, VBD, VBG, VBN, VBP, VBZ'.split(', ')\n",
    "# news_lines\n",
    "def get_content(start, batch_size):\n",
    "    return [news_metadic[news_csv.url[i]]['content'] for i in range(start, start+batch_size)]\n",
    "def train_with_feature(start, batch_size, pos_set):\n",
    "\n",
    "    batch_content = get_content(start, batch_size)\n",
    "    vocab_ref = article_content_dic(batch_content, pos_set) \n",
    "    vocab_ref_sorted = sorted(vocab_ref, key=lambda x: -vocab_ref[x])\n",
    "    vocab_index = {vocab_ref_sorted[i]:i for i in range(len(vocab_ref))}\n",
    "    \n",
    "    f_matrix = article_vocab_matrix(vocab_index, batch_content)\n",
    "    clf = MultinomialNB(alpha=1.0)\n",
    "    train_indices, test_indices = np.array(list(range(batch_size)))[:batch_size*8//10], np.array(list(range(batch_size)))[batch_size*8//10:]\n",
    "    train_matrix = f_matrix[train_indices]\n",
    "    test_matrix = f_matrix[test_indices]\n",
    "    \n",
    "    y = np.array([label(shares_lookup[news_csv.url[i]]) for i in range(start, start+batch_size)])\n",
    "    train_y, test_y = y[train_indices], y[test_indices]\n",
    "    clf.fit(train_matrix, train_y)\n",
    "    predics = clf.predict(test_matrix)\n",
    "\n",
    "    return (clf, clf.predict_log_proba(test_matrix), np.mean(1*(predics  == test_y)), vocab_index, f_matrix, y, predics)\n",
    "def find_best_features(pos_set):\n",
    "    features = []\n",
    "    while True: \n",
    "        # we only add a feature if its addition is superior to all previous feature sets\n",
    "        # and break if no such tag is found in an iteration\n",
    "        prev_score = 0\n",
    "        top_score = 0; best_tag = None\n",
    "        for tag in tags:\n",
    "            features.append(tag)\n",
    "            score = train_with_feature(batch_content, features)\n",
    "            print(\"features\", features, \"score\", score)\n",
    "            if score > top_score:\n",
    "                top_score = score\n",
    "                best_tag = tag\n",
    "            features.remove(tag)\n",
    "        if top_score <= prev_score:\n",
    "            break\n",
    "        else:\n",
    "            features.append(best_tag)\n",
    "        tags.remove(best_tag)\n",
    "        prev_score = top_score\n",
    "    return features\n",
    "def update_content_vecs(clf, start, batch_size, vocab_index, news=news_csv):\n",
    "    batch_content = get_content(start, start+batch_size)\n",
    "    naive_score = clf.predict_log_proba(article_vocab_matrix(vocab_index, batch_content))[:,0]\n",
    "    naive_col = pd.DataFrame({'naive_score': naive_score})\n",
    "    return news.iloc[start:start+batch_size,:].join(naive_col)\n",
    "def get_top_words(matrix, y, vocab_index, highest=5):\n",
    "    v = np.shape(matrix)[1]\n",
    "    labels = np.array(y)\n",
    "    pos_samples = matrix[labels == 1, :]\n",
    "    zero_samples = matrix[labels == 0, :]\n",
    "    phi_x_1 = (np.sum(pos_samples, axis = 0)+1) / (np.sum(pos_samples)+v)\n",
    "    phi_x_0 = (np.sum(zero_samples, axis = 0)+1) / (np.sum(zero_samples)+v)\n",
    "    model = np.log(phi_x_1), np.log(phi_x_0), phi_1, phi_0\n",
    "    ordered_keys = sorted(range(np.log(phi_x_1).size),\n",
    "            key = lambda i: (np.log(phi_x_1) - np.log(phi_x_0))[i], reverse = False)\n",
    "    return np.array(list(vocab_index.keys()))[ordered_keys[:highest]].tolist()\n",
    "def update_title_vecs(start,end,news=news_csv):\n",
    "    urls = news.url[start:end]\n",
    "    vecs = []\n",
    "    valid_indices = np.arange(start,end)[np.array([url in news_titles_dic for url in urls])]\n",
    "    for url in urls:\n",
    "        if url in news_titles_dic:\n",
    "            w2v = np.array([g_model[w] for w in news_titles_dic[url].lower().split() if w in g_model])\n",
    "            if not (w2v).size:\n",
    "                w2v = np.array([g_model[w] for w in url.split('/')[-2].split('-') if w in g_model])\n",
    "            vecs.append(w2v.mean(axis=0))\n",
    "    title_vecs = {}\n",
    "    for i in range(valid_indices.size):\n",
    "        title_vecs[valid_indices[i]] = vecs[i]\n",
    "\n",
    "        \n",
    "    title_col = pd.DataFrame({'title_vecs': title_vecs})\n",
    "    return news.iloc[valid_indices,:].join(title_col)                  \n",
    "def update_keyword_vecs(start,end,k_svd,news=news_csv):\n",
    "    keyword_col = pd.DataFrame({'keyword_vecs':\n",
    "        {i:k_svd[i] for i in range(start,min(end,len(news)))}\n",
    "    })\n",
    "    return news.iloc[start:end].join(keyword_col)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     8,
     27
    ]
   },
   "outputs": [],
   "source": [
    "def metrics(predics, y):\n",
    "    return {\n",
    "        \"accuracy\": (predics[y == 1].sum()+(1-predics[y == 0]).sum())/y.size,\n",
    "        \"precision\": y[predics == 1].mean(),\n",
    "        \"recall\": predics[y == 1].mean(),\n",
    "        \"specificity\": (1-predics[y == 0]).mean()\n",
    "    }\n",
    "def plot_pr(predics, y):\n",
    "    \n",
    "    average_precision = average_precision_score(t_y, t_p[:,1])\n",
    "    precision, recall, _ = precision_recall_curve(y, predics)\n",
    "    step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "              average_precision))\n",
    "    plt.show()\n",
    "def train_test_split():\n",
    "    batches = [100, 250, 500, 1000, 1500, 2000]\n",
    "    does_length_matter = {}\n",
    "    for batch_size in batches:\n",
    "        does_length_matter[batch_size] = []\n",
    "        for i in range(10000//batch_size):\n",
    "            score = train_with_feature(batch_size*i, batch_size*(i+1), ['NNS','VBN','VBG'])[2]\n",
    "            does_length_matter[batch_size].append(score)\n",
    "            print(batch_size*i, batch_size*(i+1), \":\", score)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf, probs, score, vocab_index, matrix, y, predics = train_with_feature(0, 5000, ['NNS','VBN','VBG'])\n",
    "content_updated = update_content_vecs(clf, 0,37000, vocab_index).reset_index(drop=True)\n",
    "title_updated = update_title_vecs(0,37000, content_updated).reset_index(drop=True)\n",
    "updated_csv = update_keyword_vecs(0,37000,key_tf_svd,title_updated)\n",
    "y = arr_labels(np.array(updated_csv[' shares']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# titles = np.array([news_titles_dic[url].lower() for url in news_titles_dic.keys()])\n",
    "news_content_dic = {key: news_metadic[key]['content'] for key in news_metadic.keys()}\n",
    "content=[str(news_content_dic.get(url, \"\")) for url in news_csv.url]\n",
    "titles=[news_titles_dic.get(url,\"\") for url in news_csv.url]\n",
    "# shares=[label(shares_lookup.get(url, 1)) for url in news_csv.url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# the goal here is to find optimal way to select features via lasso\n",
    "\n",
    "# C of 6.5 does the best for regular lasso\n",
    "# most coef if not all are less than 0.01 in magnitude\n",
    "\n",
    "# lr=LogisticRegression(penalty='l1',C=6.5)\n",
    "# lr.fit(wv[:-2000],y[:-2000])\n",
    "# lr.get_params\n",
    "# good_indices = np.where(np.abs(lr.coef_) > 0.4)[1]\n",
    "# lr_good=LogisticRegression(penalty='l2',C=6.5).fit(wv[:-2000,good_indices],y[:-2000])\n",
    "# f1_score(lr_good.predict(wv[-2000:,good_indices]),y[-2000:])\n",
    "\n",
    "# cutoff of 0.4 seems to prune out bad features the best\n",
    "# now we can LSA further\n",
    "\n",
    "# wv_svd=TruncatedSVD(1500,'arpack').fit_transform(wv[:,good_indices])\n",
    "\n",
    "# after searching 1500 captures > 70% of variance\n",
    "\n",
    "# summarized into a function below\n",
    "\n",
    "wv = TfidfVectorizer(lowercase=True,stop_words='english',\n",
    "                                      ngram_range=(1,6),analyzer='word',strip_accents='unicode',\n",
    "                                      min_df=1).fit_transform(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def lasso_feature_sel(content,past_start=15000,past_end=36000,C=6.5,thres=0.4,svd_dim=1500): \n",
    "    lr=LogisticRegression(penalty='l1',C=C)\n",
    "    lr.fit(wv[past_start:past_end],y[past_start:past_end])\n",
    "    good_indices = np.where(np.abs(lr.coef_) > 0.4)[1]\n",
    "    good_indices = np.where(np.abs(coef_) > thres)[1]\n",
    "    return TruncatedSVD(svd_dim,'arpack').fit_transform(wv[:,good_indices])\n",
    "\n",
    "# content_svd = lasso_feature_sel(content)\n",
    "\n",
    "# past_start=15000;past_end=36000;C=6.5\n",
    "# lr=LogisticRegression(penalty='l1',C=C)\n",
    "# lr.fit(wv[past_start:past_end],y[past_start:past_end])\n",
    "# good_indices = np.where(np.abs(lr.coef_) > 0.4)[1]\n",
    "# np.savetxt('./lr_good_indices.txt',good_indices)\n",
    "def lasso_svd_retrieve(wv,svd_dim=1500):\n",
    "    good_indices=np.loadtxt('./lr_good_indices.txt')\n",
    "    return TruncatedSVD(svd_dim,'arpack').fit_transform(wv[:,good_indices])\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3,
     16
    ]
   },
   "outputs": [],
   "source": [
    "num_batches = 39 # treating the last 644 as one batch\n",
    "# predicts content from start to end using svd-applied word-vec tfidf's\n",
    "# trained on content from past_start to past_end\n",
    "def predict_from_past(content,past_start,past_end,start,end,C,ngram):\n",
    "    past_content = content[past_start:past_end]\n",
    "    past_y = y[past_start:past_end]\n",
    "    cur_content = content[start:end]\n",
    "    cur_y = y[start:end]\n",
    "    word_vectorizer = TfidfVectorizer(lowercase=True,stop_words='english',\n",
    "                                      ngram_range=(1,ngram),analyzer='word',strip_accents='unicode',\n",
    "                                      min_df=1)\n",
    "    wv=word_vectorizer.fit(past_content)\n",
    "    lr=LogisticRegression(C=C,penalty='l1').fit(wv.transform(past_content),past_y)\n",
    "    return (lr.predict(wv.transform(cur_content)),cur_y)\n",
    "# this experiment yields how far into the past we should look to optimize the eval score\n",
    "# note the lasso_feature_sel parameters were optimized assuming we look at the entire past\n",
    "def run_experiment():\n",
    "    content_scores=[]\n",
    "    for i in reversed(range(0,36)):\n",
    "        print(\"from\",1000*i)\n",
    "        pas=predict_from_past(content,1000*i,36000,36000,37000,6.5,6)\n",
    "        print(\"f1 ba\",(f1_score(pas[0],pas[1]),balanced_accuracy_score(pas[0],pas[1])))\n",
    "        content_scores.append((f1_score(pas[0],pas[1]),balanced_accuracy_score(pas[0],pas[1])))\n",
    "        c_s = {'start':1000*np.array(list(reversed(range(i,36)))),\n",
    "         'f1_score':np.array([cs[1] for cs in content_scores]),\n",
    "         'balanced_accuracy':np.array([cs[0] for cs in content_scores])\n",
    "        }\n",
    "        \n",
    "        plt.plot(c_s['start'],c_s['f1_score'],marker='o',markerfacecolor='blue')\n",
    "        plt.plot(c_s['start'],c_s['balanced_accuracy'],marker='o',markerfacecolor='green')\n",
    "        plt.title(\"F1 and Average Recall vs Starting Batch\")\n",
    "        plt.savefig(\"./f1ba_start.png\",dpi=500)\n",
    "        plt.show()\n",
    "    return content_scores\n",
    "    \n",
    "# result: seems best to start with past_start=15000\n",
    "# max f1 is < 0.5, with low precision and recall\n",
    "\n",
    "# content_scores=run_experiment()\n",
    "\n",
    "\n",
    "# np.concatenate(updated_csv.title_vecs.values).reshape(batch_size,-1).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6
    ]
   },
   "outputs": [],
   "source": [
    "# this experiment is for how far back we should look at titles (with tfidf)\n",
    "# to maximize eval score from 36000 to 37000\n",
    "# whereas we used 6 for content ngrams, here we play by ear and use 3\n",
    "\n",
    "# title_scores=[]\n",
    "# title_models=[]\n",
    "# for i in reversed(range(36)):\n",
    "#     print(\"from\",1000*i)\n",
    "#     pas=predict_from_past(titles,1000*i,36000,36000,37000,6,3)\n",
    "#     title_scores.append((f1_score(pas[0],pas[1]),balanced_accuracy_score(pas[0],pas[1])))\n",
    "#     t_s = {'start':1000*np.array(list(reversed(range(i,36)))),\n",
    "#          'f1_score':np.array([ts[1] for ts in title_scores]),\n",
    "#          'balanced_accuracy':np.array([ts[0] for ts in title_scores])\n",
    "#         }\n",
    "#     plt.plot(t_s['start'],t_s['f1_score'],marker='o',markerfacecolor='blue')\n",
    "#     plt.plot(t_s['start'],t_s['balanced_accuracy'],marker='o',markerfacecolor='green')\n",
    "#     plt.title(\"F1 and Average Recall vs Starting Batch -- Titles\")\n",
    "#     plt.savefig(\"./f1ba_start_titles.png\",dpi=500)\n",
    "#     plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3,
     14,
     24
    ]
   },
   "outputs": [],
   "source": [
    "# construct transformed tfidf matrix\n",
    "# perform SVD and return best features\n",
    "# will be used for titles as well as content\n",
    "def reduce_to_best(titles,start,batch_size,past_start,past_end=36000,\n",
    "                   ngram=2,trunc_size=1000):\n",
    "    # from experimenting above, only past_start=27000 to past_end=36000 is needed\n",
    "    wv=TfidfVectorizer(lowercase=True,stop_words='english',\n",
    "                                      ngram_range=(1,ngram),analyzer='word',strip_accents='unicode',\n",
    "                                      min_df=1).fit(titles[past_start:past_end])\n",
    "    wv=wv.transform(titles[start:start+batch_size])\n",
    "    svd=TruncatedSVD(trunc_size,'arpack').fit(wv)\n",
    "    print(np.sum(svd.explained_variance_ratio_))\n",
    "    X=svd.transform(wv)\n",
    "    return X/np.max(np.abs(X),axis=0)    \n",
    "def get_features(updated_csv,start,batch_size):\n",
    "    feature_vecs = updated_csv.drop([' shares','title_vecs','keyword_vecs'],axis=1).iloc[:,1:].values\n",
    "    t_vecs = np.concatenate(updated_csv.title_vecs.values).reshape(batch_size,-1)\n",
    "    k_vecs = np.concatenate(updated_csv.keyword_vecs.values).reshape(batch_size,-1)\n",
    "    feature_vecs = np.hstack((feature_vecs, t_vecs, k_vecs))\n",
    "    max_observed = np.max(np.abs(feature_vecs),axis=0)\n",
    "    return np.hstack((feature_vecs/max_observed, \n",
    "                      reduce_to_best(titles,start,batch_size,27000),\n",
    "                      lasso_svd_retrieve(wv[start:start+batch_size,:])))\n",
    "# assumes y has two labels!\n",
    "def train_val_test_split(x, y, split=0.8):\n",
    "    pos_x = x[y==1]; pos_y = y[y==1]\n",
    "    neg_x = x[y==0]; neg_y = y[y==0]\n",
    "    pos_perm = np.random.permutation(pos_x.shape[0])\n",
    "    neg_perm = np.random.permutation(neg_x.shape[0])\n",
    "    pos_x = pos_x[pos_perm]; pos_y = pos_y[pos_perm]\n",
    "    neg_x = neg_x[neg_perm]; neg_y = neg_y[neg_perm]\n",
    "\n",
    "    ro = lambda x: int(x*split)+1\n",
    "    return {'x_train': np.append(pos_x[:ro(pos_y.size)], neg_x[:ro(neg_y.size)], axis=0),\n",
    "            'y_train': np.append(pos_y[:ro(pos_y.size)], neg_y[:ro(neg_y.size)]),\n",
    "            'x_test': np.append(pos_x[ro(pos_y.size):], neg_x[ro(neg_y.size):], axis=0),\n",
    "            'y_test': np.append(pos_y[ro(pos_y.size):], neg_y[ro(neg_y.size):])}\n",
    "\n",
    "norm_features = get_features(updated_csv,0,37000)\n",
    "y = arr_labels(np.array(updated_csv[' shares']))\n",
    "data = {'x_train':norm_features[:36000],\n",
    "        'x_eval':norm_features[36000:],\n",
    "        'y_train':y[:36000],\n",
    "        'y_eval':y[36000:]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# anatomy of x's 3610 features is as follows:\n",
    "# first 60 are original numerical features\n",
    "# next 100 are title and keyword word2vec\n",
    "# next 1000 are title vecs\n",
    "# final 1500 is tfidf features after lasso and lsa\n",
    "\n",
    "# a suitable kernel is:\n",
    " \n",
    "def article_kernel(X1,X2,k1,k2):\n",
    "    p_k1=pairwise_kernels(X1[:,:60],X2[:,:60].T,metric=k1)\n",
    "    w2v=np.dot(X1[:,60:160],X2[:,60:160].T)\n",
    "    p_k2=pairwise_kernels(X1[:,-1500:],X2[:,-1500:].T,metric=k2)\n",
    "    return p_k1+w2v+p_k2\n",
    "    \n",
    "\n",
    "# data['x_train'].shape\n",
    "\n",
    "data['x_train']=np.hstack((data['x_train'][:,:160],data['x_train'][:,-1500:]))\n",
    "data['x_eval']=np.hstack((data['x_eval'][:,:160],data['x_eval'][:,-1500:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2,
     9,
     40
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evalu = {}\n",
    "\n",
    "def title_w2v_baseline(data, vec_len=50):\n",
    "    gram_train = pairwise_kernels(data['x_train'][:,-vec_len:], metric=\"cosine\")\n",
    "    gram_test = pairwise_kernels(data['x_test'][:,-vec_len:], metric=\"cosine\")\n",
    "    svc = SVC(kernel=\"precomputed\")\n",
    "    svc.fit(gram_train, data['y_train'])\n",
    "    return metrics(svc.predict(gram_train), data['y_train'])\n",
    "\n",
    "def train_with_svc(data, evalu):\n",
    "    svc = SVC(kernel=\"linear\",verbose=10)\n",
    "    svc.fit(data['x_train'], data['y_eval'])\n",
    "    predics = svc.predict(data['x_eval'])\n",
    "    evalu[\"linear\"] = metrics(predics, data['y_eval'])\n",
    "    \n",
    "    gram_train = pairwise_kernels(data['x_train'], metric=\"cosine\")\n",
    "    gram_test = pairwise_kernels(data['x_eval'], metric=\"cosine\")\n",
    "    svc = SVC(kernel=\"precomputed\")\n",
    "    svc.fit(gram_train, data['y_train'])\n",
    "    evalu[\"cosine\"] = metrics(svc.predict(gram_train), data['y_train'])\n",
    "    \n",
    "    # param_grid = {\"gamma\": np.logspace(-10, 10, 11)}\n",
    "    # svc = GridSearchCV(SVC(kernel=\"rbf\"), param_grid=param_grid, cv=5)\n",
    "    # svc.fit(data['x_train'], data['y_train'])\n",
    "    # to save time, this grid search gives gamma=1e-4\n",
    "    svc = SVC(kernel=\"rbf\", gamma=1e-4,verbose=10)\n",
    "    svc.fit(data['x_train'], data['y_train'])\n",
    "    predics = svc.predict(data['x_eval'])\n",
    "    evalu[\"rbf\"] = metrics(predics, data['y_eval'])\n",
    "    \n",
    "    # found after trial and error with np.logspace\n",
    "    # param_grid={\"degree\":np.linspace(2.5,3.5,3),\"coef0\":np.linspace(1.5,2.5,5),\"gamma\":np.linspace(0.02,0.04,5)}\n",
    "    # svc = GridSearchCV(SVC(kernel=\"poly\"),verbose=10,param_grid=param_grid,scoring='balanced_accuracy',cv=5)\n",
    "    svc = SVC(kernel=\"poly\", gamma=0.03, coef0=2.25, degree=3,verbose=10)\n",
    "    svc.fit(data['x_train'], data['y_train'])\n",
    "    predics = svc.predict(data['x_eval'])\n",
    "    evalu[\"poly\"] = metrics(predics, data['y_eval'])\n",
    "    \n",
    "    return\n",
    "\n",
    "def train_with_lr(data,reg=2.0):\n",
    "    # regularized logistic regression on new data with titlevec\n",
    "    # mlp = GridSearchCV(LogisticRegression(),\n",
    "    #                    param_grid={\"C\":np.linspace(1,2,5),'solver':['liblinear','lbfgs']},cv=3,verbose=10,\n",
    "    #                    scoring=\"f1_micro\")\n",
    "\n",
    "    # best one is 1.0, l1\n",
    "    mlpl1 = LogisticRegression(C=reg,verbose=10,solver='saga',penalty='l1')\n",
    "    mlpl2 = LogisticRegression(C=reg,verbose=10,solver='saga',penalty='l2')\n",
    "    mlpl1.fit(data['x_train'],data['y_train'])\n",
    "    mlpl2.fit(data['x_train'],data['y_train'])\n",
    "    return mlpl1,mlpl2\n",
    "\n",
    "# for reg in np.linspace(1,3,5):\n",
    "#     mlpl1,mlpl2 = train_with_lr(data,reg=reg)\n",
    "#     print(reg)\n",
    "#     print(metrics(mlpl1.predict(data['x_eval']),data['y_eval']))\n",
    "#     print(metrics(mlpl2.predict(data['x_eval']),data['y_eval']))\n",
    "\n",
    "\n",
    "kerns = ['additive_chi2', 'chi2', 'linear', 'poly', 'polynomial', 'rbf',\n",
    "'laplacian', 'sigmoid', 'cosine']\n",
    "for k1 in kerns:\n",
    "    for k2 in kerns:\n",
    "        print(\"k1\",k1,\"k2\",k2)\n",
    "        svc = SVC(kernel=article_kernel(k1,k2),gamma=1e-4)\n",
    "        svc.fit(data['x_train'],data['y_train'])\n",
    "        print(metrics(svc.predict(data['x_eval']),data['y_eval']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# random forests\n",
    "\n",
    "rfc_grid={}\n",
    "# 5 is approx s\n",
    "# we should have limited depth, as otherwise rfc will abuse the tfidf/lsa features\n",
    "# we start conservative with low bias\n",
    "rfc = RandomForestClassifier(n_estimators=100,verbose=10)\n",
    "rfc.fit(data['x_train'],data['y_train'])\n",
    "f1_score(rfc.predict(data['x_eval']),data['y_eval'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "good_news_venv",
   "language": "python",
   "name": "good_news_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
